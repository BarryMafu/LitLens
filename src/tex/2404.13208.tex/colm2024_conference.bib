
@String{aaai="Association for the Advancement of Artificial Intelligence"}
@String{aaim="International Conference on Algorithmic Aspects in Information and Management"}
@String{acl="Proceedings of the Association for Computational Linguistics"}
@String{aistats="Proceedings of Artificial Intelligence and Statistics"}
@String{cl="Computational Linguistics"}
@String{chi="International Conference on Human Factors in Computing Systems"}
@String{iui="International Conference on Intelligent User Interfaces"}
@String{cvpr="Computer Vision and Pattern Recognition"}
@String{coling="Proceedings of International Conference on Computational Linguistics"}
@String{colt="Proceedings of Conference on Learning Theory"}
@String{conll="Conference on Computational Natural Language Learning"}
@String{eacl="Proceedings of the European Chapter of the Association for Computational Linguistics"}
@String{ecir="Proceedings of the European Conference on Information Retrieval"}
@String{ecml="Proceedings of European Conference of Machine Learning"}
@String{emnlp="Proceedings of Empirical Methods in Natural Language Processing"}
@String{emnlpdemo="Proceedings of Empirical Methods in Natural Language Processing: System Demonstrations"}
@String{hlt="Proceedings of the Human Language Technology Conference"}
@String{ica="International Conference on Independent Component Analysis and Signal Separation"}
@String{icdm="International Conference on Data Mining"}
@String{iccv="International Conference on Computer Vision"}
@String{eccv="European Conference on Computer Vision"}
@String{iclsp="International Conference on Lexicography and Semantic Prosody"}
@String{icslp="International Conference on Spoken Language Processing"}
@String{icwsm="International Conference on Weblogs and Social Media"}
@String{ai="Artificial Intelligence"}
@String{ijcai="International Joint Conference on Artificial Intelligence"}
@String{ijcnlp="International Joint Conference on Natural Language Processing"}
@String{ijcv="International Journal of Computer Vision"}
@String{iclr="ICLR"}
@String{icml="ICML"}
@String{ismir="International Society for Music Information Retrieval Conference"}
@String{jacm="Journal of the Association for Computing Machinery"}
@String{jair="Journal of Artificial Intelligence Research"}
@String{jasa="Journal of the American Statistical Association"}
@String{lrec="Proceedings of the Language Resources and Evaluation Conference"}
@String{kdd="Knowledge Discovery and Data Mining"}
@String{jmlr="Journal of Machine Learning Research"}
@String{mlj="Machine Learning"}
@String{cikm="Proceedings of the ACM International Conference on Information and Knowledge Management"}
@String{naacl="Conference of the North American Chapter of the Association for Computational Linguistics"}
@String{neurips="Proceedings of Advances in Neural Information Processing Systems"}
@String{osdi="Symposium on Operating System Design and Implementation"}
@String{pami="IEEE Transactions on Pattern Analysis and Machine Intelligence"}
@String{pnas="Proceedings of the National Academy of Sciences"}
@String{sdm="SIAM on Data Mining"}
@String{semeval="Proceedings of the Workshop on Semantic Evaluation"}
@String{sigir="Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval"}
@String{starsem="Proceedings of the Joint Conference on Lexical and Computational Semantics"}
@String{ecir="Proceedings of the European Conference on Information Retrieval"}
@String{uai="Proceedings of Uncertainty in Artificial Intelligence"}
@String{vldb="International Conference on Very Large Databases"}
@String{www="Proceedings of the World Wide Web Conference"}
@String{icassp="IEEE International Conference on Acoustics, Speech, and Signal Processing"}
@String{ica="International Conference on Independent Component Analysis and Signal Separation"}
@String{sdm="Proceedings of SIAM International Conference on Data Mining"}
@String{tacl="TACL"}
@String{taslp="Transactions on Audio, Speech, and Language Processing"}
@String{wsdm="Proceedings of ACM International Conference on Web Search and Data Mining"}
@String{jrssb="Journal of the Royal Statistical Society: Series B"}
% These are hardly ever used
@String{nodalida="Proceedings of the Nordic Conference on Computational Linguistics"}
@String{adcs="Proceedings of the Aurstralasian Document Computing Symposium"}
@String{sigdial="Proceedings of the Annual SIGDIAL Meeting on Discourse and Dialogue"}
@String{interspeech="Proceedings of the Annual Conference of the International Speech Communication Association"}
@String{asru="Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop"}
@String{iwslt="Proceedings of the International Workshop on Spoken Language Translation"}
@String{eamt="Proceedings of the European Association for Machine Translation"}
@String{phyrev="Physical Review E"}
@String{sem="Proceedings of the Joint Conference on Lexical and Computational Semantics"}
@String{cscw="Conference on Computer Supported Cooperative Work and Social Computing"}
@String{rep4nlp="Proceedings of the Workshop on Representation Learning for NLP"}
@String{ssst="Proceedings of the Workshop on Syntax, Semantics and Structure in Statistical Translation"}
@String{akbc="Proceedings of the Conference on Automated Knowledge Base Construction"}
@String{tmlr="Transactions on Machine Learning Research"}




@misc{chatgpt,
    author = {OpenAI},
    title = {{ChatGPT} blog post},
    year = {2022},
    howpublished = {\url{https://openai.com/blog/chatgpt}},
}
@misc{bff,
    author = { Groeneveld, Dirk },
    title = { The Big Friendly Filter },
    year = { 2023 },
    howpublished = {\url{https://github.com/allenai/bff}},
}




@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle=icml,
  year={2017}
}

@inproceedings{dwork2006calibrating,
  title={Calibrating noise to sensitivity in private data analysis},
  author={Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  booktitle={Theory of Cryptography},
  year={2006},
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={ACM SIGSAC},
  year={2016},
}

@inproceedings{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam  and others},
      year={2020},
      booktitle={NeurIPS}
}

@inproceedings{
    khandelwal2020generalization,
    title={Generalization through Memorization: Nearest Neighbor Language Models},
    author={Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and Mike Lewis},
    booktitle=iclr,
    year={2020}
}

@article{vyas2023provable,
  title={Provable copyright protection for generative models},
  author={Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
  booktitle=icml,
  year={2023}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle=icml,
  year={2022},
}

@inproceedings{zhong2022training,
  title={Training Language Models with Memory Augmentation},
  author={Zhong, Zexuan and Lei, Tao and Chen, Danqi},
  booktitle=emnlp,
  year={2022}
}

@inproceedings{
ouyang2022training,
title={Training language models to follow instructions with human feedback},
author={Long Ouyang and Jeffrey Wu and Xu Jiang and Diogo Almeida and Carroll Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Gray and John Schulman and others},
booktitle=neurips,
year={2022}
}

@inproceedings{kakol2013subjectivity,
author = {Kakol, Michal and Jankowski-Lorek, Micha\l{} and Abramczuk, Katarzyna and Wierzbicki, Adam and Catasta, Michele},
title = {On the Subjectivity and Bias of Web Content Credibility Evaluations},
year = {2013},
booktitle = {WWW},
}

@techreport{peS2o,
    author = {Luca Soldaini and Kyle Lo},
    year = 2023,
    title = {{peS2o (Pretraining Efficiently on S2ORC) Dataset}},
    institution = {{Allen Institute for AI}},
    note = {ODC-By, \url{https://github.com/allenai/pes2o}}
}

@article{zhang2023right,
  title={Right to be Forgotten in the Era of Large Language Models: Implications, Challenges, and Solutions},
  author={Zhang, Dawen and Finckenberg-Broman, Pamela and Hoang, Thong and Pan, Shidong and Xing, Zhenchang and Staples, Mark and Xu, Xiwei},
  journal={arXiv preprint arXiv:2307.03941},
  year={2023}
}

@misc{bourtoule2020machine,
      title={Machine Unlearning}, 
      author={Lucas Bourtoule and Varun Chandrasekaran and Christopher A. Choquette-Choo and Hengrui Jia and Adelin Travers and Baiwu Zhang and David Lie and Nicolas Papernot},
      year={2020},
      eprint={1912.03817},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {``}bad{''} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.",
}


@article{brittain2023copyright,
  title={{U.S.} Copyright Office says some {AI}-assisted works may be copyrighted},
  author={Blake Brittain},
  journal={Reuters},
  year={2023}
}

@article{henderson2023foundation,
  title={Foundation models and fair use},
  author={Henderson, Peter and Li, Xuechen and Jurafsky, Dan and Hashimoto, Tatsunori and Lemley, Mark A and Liang, Percy},
  journal={arXiv preprint arXiv:2303.15715},
  year={2023},
}

@article{henderson2022pile,
  title={Pile of {Law}: Learning responsible data filtering from the law and a {256GB} open-source legal dataset},
  author={Henderson, Peter and Krass, Mark and Zheng, Lucia and Guha, Neel and Manning, Christopher D and Jurafsky, Dan and Ho, Daniel},
  journal=neurips,
  year={2022}
}

@misc{golatkar2023training,
      title={Training Data Protection with Compositional Diffusion Models}, 
      author={Aditya Golatkar and Alessandro Achille and Ashwin Swaminathan and Stefano Soatto},
      year={2023},
      eprint={2308.01937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{liu-etal-2021-dexperts,
    title = "{DE}xperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts",
    author = "Liu, Alisa  and
      Sap, Maarten  and
      Lu, Ximing  and
      Swayamdipta, Swabha  and
      Bhagavatula, Chandra  and
      Smith, Noah A.  and
      Choi, Yejin",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.522",
    doi = "10.18653/v1/2021.acl-long.522",
    pages = "6691--6706",
    abstract = "Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with {``}expert{''} LMs and/or {``}anti-expert{''} LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.",
}

@inproceedings{pfeiffer2022lifting,
    title = "Lifting the Curse of Multilinguality by Pre-training Modular Transformers",
    author = "Pfeiffer, Jonas  and
      Goyal, Naman  and
      Lin, Xi  and
      Li, Xian  and
      Cross, James  and
      Riedel, Sebastian  and
      Artetxe, Mikel",
    booktitle =naacl,
    year = "2022",
}

@article{villalobos2022run,
      title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning}, 
      author={Pablo Villalobos and Jaime Sevilla and Lennart Heim and Tamay Besiroglu and Marius Hobbhahn and Anson Ho},
      year={2022},
      eprint={2211.04325},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
fried2023incoder,
title={InCoder: A Generative Model for Code Infilling and Synthesis},
author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Scott Yih and Luke Zettlemoyer and Mike Lewis},
booktitle=iclr,
year={2023},
}

@article{zhang2021counterfactual,
  title={Counterfactual memorization in neural language models},
  author={Zhang, Chiyuan and Ippolito, Daphne and Lee, Katherine and Jagielski, Matthew and Tram{\`e}r, Florian and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2112.12938},
  year={2021}
}

@article{feldman2020neural,
  title={What neural networks memorize and why: Discovering the long tail via influence estimation},
  author={Feldman, Vitaly and Zhang, Chiyuan},
  journal=neurips,
  year={2020}
}

@misc{pfeiffer2020madx,
      title={MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer}, 
      author={Jonas Pfeiffer and Ivan Vulić and Iryna Gurevych and Sebastian Ruder},
      year={2020},
      eprint={2005.00052},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{jang2023exploring,
      title={Exploring the Benefits of Training Expert Language Models over Instruction Tuning}, 
      author={Joel Jang and Seungone Kim and Seonghyeon Ye and Doyoung Kim and Lajanugen Logeswaran and Moontae Lee and Kyungjae Lee and Minjoon Seo},
      year={2023},
      booktitle=icml,
}


@misc{chen2022modsquad,
      title={Mod-Squad: Designing Mixture of Experts As Modular Multi-Task Learners}, 
      author={Zitian Chen and Yikang Shen and Mingyu Ding and Zhenfang Chen and Hengshuang Zhao and Erik Learned-Miller and Chuang Gan},
      year={2022},
      eprint={2212.08066},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{gururangan2023scaling,
      title={Scaling Expert Language Models with Unsupervised Domain Discovery}, 
      author={Suchin Gururangan and Margaret Li and Mike Lewis and Weijia Shi and Tim Althoff and Noah A. Smith and Luke Zettlemoyer},
      year={2023},
      eprint={2303.14177},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}


@inproceedings{gururangan-etal-2022-demix,
    title = "{DEM}ix Layers: Disentangling Domains for Modular Language Modeling",
    author = "Gururangan, Suchin  and
      Lewis, Mike  and
      Holtzman, Ari  and
      Smith, Noah A.  and
      Zettlemoyer, Luke",
    booktitle = naacl,
    year = "2022",
}]
@inproceedings{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  booktitle=icml,
  year={2022},
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{carlini2021extracting,
  title={Extracting Training Data from Large Language Models.},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom B and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={USENIX Security Symposium},
  year={2021}
}

@inproceedings{carlini2023extracting,
  title={Extracting training data from diffusion models},
  author={Carlini, Nicholas and Hayes, Jamie and Nasr, Milad and Jagielski, Matthew and Sehwag, Vikash and Tramer, Florian and Balle, Borja and Ippolito, Daphne and Wallace, Eric},
  booktitle={USENIX Security Symposium},
  year={2023}
}

@article{xie2023doremi,
  title={{DoReMi}: Optimizing Data Mixtures Speeds Up Language Model Pretraining},
  author={Xie, Sang Michael and Pham, Hieu and Dong, Xuanyi and Du, Nan and Liu, Hanxiao and Lu, Yifeng and Liang, Percy and Le, Quoc V and Ma, Tengyu and Yu, Adams Wei},
  journal={arXiv preprint arXiv:2305.10429},
  year={2023}
}

@inproceedings{lee2021deduplicating,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle=acl,
  year={2022}
}

@article{chang2023speak,
  title={Speak, memory: An archaeology of books known to {ChatGPT}/{GPT}-4},
  author={Chang, Kent K and Cramer, Mackenzie and Soni, Sandeep and Bamman, David},
  journal={arXiv preprint arXiv:2305.00118},
  year={2023}
}


@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle=iclr,
  year={2023},
}

@article{gao2020pile,
  title={The {Pile}: An {800GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020},
}

@inproceedings{bourtoule2021machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={IEEE S\&P},
  year={2021},
}

@misc{together2023redpajama,
  author = {Together},
  title = {{RedPajama}: An Open Source Recipe to Reproduce {LLaMA} training dataset},
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle=cvpr,
  year={2022}
}


@article{wu2022membership,
  title={Membership Inference Attacks Against Text-to-image Generation Models},
  author={Wu, Yixin and Yu, Ning and Li, Zheng and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2210.00968},
  year={2022}
}

@article{hu2023membership,
  title={Membership inference of diffusion models},
  author={Hu, Hailong and Pang, Jun},
  journal={arXiv preprint arXiv:2301.09956},
  year={2023}
}

@inproceedings{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  booktitle=iclr,
  year={2023}
}

@article{huang2023privacy,
  title={Privacy Implications of Retrieval-Based Language Models},
  author={Huang, Yangsibo and Gupta, Samyak and Zhong, Zexuan and Li, Kai and Chen, Danqi},
  journal={arXiv preprint arXiv:2305.14888},
  year={2023},
}


@article{lemley2020fair,
  title={Fair learning},
  author={Lemley, Mark A and Casey, Bryan},
  journal={Texas Law Review},
  year={2020},
  publisher={HeinOnline}
}

@inproceedings{gao2023pal,
  title={{PAL}: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={ICML},
  year={2023},
}

@inproceedings{mialon2023augmented,
  title={Augmented language models: {A} survey},
  author={Mialon, Gr{\'e}goire and Dess{\`\i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and Dwivedi-Yu, Jane and Celikyilmaz, Asli and others},
  booktitle={TMLR},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@misc{AutoGPT,
  author = {Toran Bruce Richards},
  title = {{AutoGPT}},
  year = {2023},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/Significant-Gravitas/AutoGPT}},
}

@inproceedings{chowdhery2022palm,
  title={{PaLM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  booktitle={JMLR},
  year={2023}
}

@article{muennighoff2023scaling,
      title={Scaling Data-Constrained Language Models}, 
      author={Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
      year={2023},
      journal={arXiv preprint arXiv:2305.16264}
}


@article{black2022gptneox20b,
      title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model}, 
      author={Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach},
      year={2022},
      journal={arXiv preprint arXiv:2204.06745},
}

@article{touvron2023llama,
      title={{LLaMA}: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      journal={arXiv preprint arXiv:2302.13971}
}


@inproceedings{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      booktitle=neurips,
}


@inproceedings{palenmichel2022multilingual,
    title = "Multilingual Open Text Release 1: Public Domain News in 44 Languages",
    author = "Palen-Michel, Chester  and
      Kim, June  and
      Lignos, Constantine",
    booktitle = lrec,
    year = "2022",
}

@article{al2023ab,
  title={The (ab)use of Open Source Code to Train Large Language Models},
  author={Al-Kaswan, Ali and Izadi, Maliheh},
  journal={arXiv preprint arXiv:2302.13681},
  year={2023}
}


@misc{gutenberg,
      title={Project Gutenberg}, 
      author={{Project Gutenberg}},
      url = {www.gutenberg.org},
}

@misc{arxiv_dataset,
	title={arXiv Dataset},
        author="ArXiv",
        year=2023,
	url={https://www.kaggle.com/dsv/6015950},
	DOI={10.34740/KAGGLE/DSV/6015950}}
 
@inproceedings{lo-etal-2020-s2orc,
    title = "{S}2{ORC}: The Semantic Scholar Open Research Corpus",
    author = "Lo, Kyle  and
      Wang, Lucy Lu  and
      Neumann, Mark  and
      Kinney, Rodney  and
      Weld, Daniel",
    booktitle = acl,
    year = "2020"
}

@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal=neurips,
  year={2021}
}

@inproceedings{saxton2019analysing,
      title={Analysing Mathematical Reasoning Abilities of Neural Models}, 
      author={David Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},
      year={2019},
      booktitle=iclr,
}


@misc{office2023,
    author={{U.S.} {Copyright Office}},
    year={2023},
    title={{FAQ}},
}


@misc{meta2023,
    author={Yu, Lili and Shi, Bowen and Pasunuru, Ram and Miller, Benjamin},
    year={2023},
    title={{Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning}},
}

@misc{office2021,
    author={U.S. Copyright Office},
    year={2021},
    title={More Information on Fair Use},
}

@misc{caselaw2018,
      title = {Caselaw Access Project},
      author ={{Caselaw Access Project}},
      publisher = "Harvard",
      url = "https://case.law/"
}

@misc{gershgorn2021,
    author={David Gershgorn},
    year={2021},
    title={GitHub’s Automatic Coding Tool Rests on Untested Legal Ground},
    url={https://www.theverge.com/
2021/7/7/22561180/github-copilot-legal-copyright-fair-use-public-code}}

@misc{vincent2023,
    author={James Vincent},
    year={2023},
    title={Getty Images sues {AI} art generator Stable Diffusion in the US for copyright infringement},
    url={https://www.theverge.com/2023/2/6/23587393/ai-art-copyright-lawsuit-getty-images-stable-diffusion}}

@misc{de2023chatgpt,
    author={De Vynck, Gerrit},
    year={2023},
    title={{ChatGPT} maker {OpenAI} faces a lawsuit over how it used people’s data},
    url={https://www.washingtonpost.com/technology/2023/06/28/openai-chatgpt-lawsuit-class-action/}
}

@misc{jlversusalphabet,
    author={{J.L. et al. v. Alphabet Inc.}},
    year={2023},
    title={Case 3:23-cv-03416, {N.D. Cal.}},
    url={https://storage.courtlistener.com/recap/gov.uscourts.cand.415223/gov.uscourts.cand.415223.1.0.pdf}
}

@misc{silvermanmeta,
    author={{Silverman et al. v. Meta Platforms, Inc.}},
    year={2023},
    title={Case 3:23-cv-03417, {N.D. Cal.}},
    url={https://storage.courtlistener.com/recap/gov.uscourts.cand.415175/gov.uscourts.cand.415175.1.0.pdf}
}

@misc{silverman,
    author={{Silverman et al. v. OpenAI, Inc.}},
    year={2023},
    title={Case 3:23-cv-03417, {N.D. Cal.}},
    url={https://storage.courtlistener.com/recap/gov.uscourts.cand.415174/gov.uscourts.cand.415174.1.0_1.pdf}
}

@misc{tremblay,
    author={Tremblay et al. v. OpenAI, Inc.},
    year={2023},
    title={Case 3:23-cv-03223 , {N.D. Cal.}},
    url={https://storage.courtlistener.com/recap/gov.uscourts.cand.414822/gov.uscourts.cand.414822.1.0.pdf}
}

@misc{pmversusopenai,
    author={P.M. et al. v. OpenAI, Inc.},
    year={2023},
    title={Case 3:23-cv-03199-JCS, {N.D. Cal.}},
    url={https://storage.courtlistener.com/recap/gov.uscourts.cand.414754/gov.uscourts.cand.414754.1.0.pdf}
}



@misc{dave2023,
    author={Paresh Dave},
    year={2023},
    title={Stack Overflow Will Charge {AI} Giants for Training Data},
    url={https://www.wired.com/story/stack-overflow-will-charge-ai-giants-for-training-data/}}

@misc{isaac2023,
    author={Mike Isaac},
    year={2023},
    title={Reddit Wants to Get Paid for Helping to Teach Big {A.I.} Systems},
    url={https://www.nytimes.com/2023/04/18/technology/reddit-ai-openai-google.html?smid=url-share}}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{metz2022lawsuit,
  title={Lawsuit Takes Aim at the Way {A.I.} Is Built},
  author={Cade Metz},
  journal={New York Times},
  year={2022}
}

@article{bing,
  title={The new {Bing} and {Edge} --- progress
from our first month},
  author={Yusuf Mehdi},
  journal={Bing search blog},
  year={2023}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={JMLR},
  year={2020}
}

@inproceedings{
bandy2021addressing,
title={Addressing ``Documentation Debt''' in Machine Learning: A Retrospective Datasheet for {BookCorpus}},
author={Jack Bandy and Nicholas Vincent},
booktitle=neurips,
year={2021},
}

@article{sobel2017artificial,
  title={Artificial Intelligence's Fair Use Crisis},
  author={Sobel, Benjamin LW},
  journal={Columbia Journal of Law \& the Arts},
  year={2017},
  publisher={HeinOnline}
}

@article{levendowski2018copyright,
  title={How copyright law can fix artificial intelligence's implicit bias problem},
  author={Levendowski, Amanda},
  journal={Washington Law Review},
  year={2018},
  publisher={HeinOnline}
}

@inproceedings{min2022nonparametric,
  title={Nonparametric Masked Language Modeling},
  author={Min, Sewon and Shi, Weijia and Lewis, Mike and Chen, Xilun and Yih, Wen-tau and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Findings of ACL},
  year={2023}
}

@article{shi2023replug,
  title={{REPLUG}: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}

@article{ram2023context,
  title={In-context retrieval-augmented language models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal=tacl,
  year={2023}
}

@article{huang2023k,
  title={{$k$NN-Adapter}: Efficient Domain Adaptation for Black-Box Language Models},
  author={Huang, Yangsibo and Liu, Daogao and Zhong, Zexuan and Shi, Weijia and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2302.10879},
  year={2023}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle=icml,
  year={2020},
}

@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  year={2019},
}

@inproceedings{lan2023copy,
  title={Copy is all you need},
  author={Lan, Tian and Cai, Deng and Wang, Yan and Huang, Heyan and Mao, Xian-Ling},
  booktitle=iclr,
  year={2023}
}

@inproceedings{he-etal-2021-efficient,
    title = "Efficient Nearest Neighbor Language Models",
    author = "He, Junxian  and
      Neubig, Graham  and
      Berg-Kirkpatrick, Taylor",
    booktitle = emnlp,
    year = "2021",
}

@inproceedings{alon2022neuro,
  title={Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval},
  author={Alon, Uri and Xu, Frank and He, Junxian and Sengupta, Sudipta and Roth, Dan and Neubig, Graham},
  booktitle={ICML},
  year={2022}
}

@article{jiang2023active,
  title={Active retrieval augmented generation},
  author={Jiang, Zhengbao and Xu, Frank F and Gao, Luyu and Sun, Zhiqing and Liu, Qian and Dwivedi-Yu, Jane and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2305.06983},
  year={2023}
}
@inproceedings{mackenzie2020cc,
  title={{CC-News-En}: A large English news corpus},
  author={Mackenzie, Joel and Benham, Rodger and Petri, Matthias and Trippas, Johanne R and Culpepper, J Shane and Moffat, Alistair},
  booktitle=cikm,
  year={2020}
}
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={EMNLP},
  year={2013}
}
@article{johnson2016mimic,
  title={{MIMIC-III}, a freely accessible critical care database},
  author={Johnson, Alistair EW and Pollard, Tom J and Shen, Lu and Lehman, Li-wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G},
  journal={Scientific data},
  year={2016}
}

@article{mireshghallah2022quantifying,
  title={Quantifying privacy risks of masked language models using membership inference attacks},
  author={Mireshghallah, Fatemehsadat and Goyal, Kartik and Uniyal, Archit and Berg-Kirkpatrick, Taylor and Shokri, Reza},
  journal={arXiv preprint arXiv:2203.03929},
  year={2022}
}

@inproceedings{chen2022re,
  title={{Re-Imagen}: Retrieval-augmented text-to-image generator},
  author={Chen, Wenhu and Hu, Hexiang and Saharia, Chitwan and Cohen, William W},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{dodge2021documenting,
  title={Documenting large webtext corpora: A case study on the colossal clean crawled corpus},
  author={Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
  booktitle=emnlp,
  year={2021}
}


@inproceedings{dodge2021documenting,
  title={Documenting large webtext corpora: A case study on the colossal clean crawled corpus},
  author={Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
  booktitle=emnlp,
  year={2021}
}

@inproceedings{zhang2021situatedqa,
  title={{SituatedQA}: Incorporating extra-linguistic contexts into {QA}},
  author={Zhang, Michael JQ and Choi, Eunsol},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{du2022synthetic,
  title={Synthetic disinformation attacks on automated fact verification systems},
  author={Du, Yibing and Bosselut, Antoine and Manning, Christopher D},
  booktitle={AAAI},
  year={2022}
}


@article{bush2019bing,
  title={Bing’s Top Search Results
Contain an Alarming Amount of Disinformation},
  author={Daniel Bush and Alex Zaheer},
  journal={Internet Observatory News},
  year={2019}
}

@inproceedings{abdelnabi2023fact,
  title={{Fact-Saboteurs:} {A} Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems},
  author={Abdelnabi, Sahar and Fritz, Mario},
  booktitle={USENIX},
  year={2023}
}

@inproceedings{rao-daume-iii-2018-learning,
    title = "Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information",
    author = "Rao, Sudha  and Daum{\'e} III, Hal",
    booktitle = "ACL",
    year = {2018},
}

@inproceedings{chen2017reading,
  title={Reading wikipedia to answer open-domain questions},
  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  booktitle={ACL},
  year={2017}
}

@inproceedings{chen2022rich,
  title={Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence},
  author={Chen, Hung-Ting and Zhang, Michael JQ and Choi, Eunsol},
  booktitle={EMNLP},
  year={2022}
}

@inproceedings{ijcai2017p570,
  author    = {Isaac Persing and Vincent Ng},
  title     = {Why Can't You Convince Me? {Modeling} Weaknesses in Unpersuasive Arguments},
  booktitle = {IJCAI},
  year      = {2017},
}

@inproceedings{toledo-etal-2019-automatic,
    title = "Automatic Argument Quality Assessment---{New} Datasets and Methods",
    booktitle = "EMNLP",
    author = "Toledo, Assaf  and
      Gretz, Shai  and
      Cohen-Karlik, Edo  and
      Friedman, Roni  and
      Venezian, Elad  and
      Lahav, Dan  and
      Jacovi, Michal  and
      Aharonov, Ranit  and
      Slonim, Noam",
    year = {2019},
}

@inproceedings{izacard2020leveraging,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  booktitle={EACL},
  year={2021}
}

@inproceedings{mochales2009,
author = {Palau, Raquel Mochales and Moens, Marie-Francine},
title = {Argumentation Mining: The Detection, Classification and Structure of Arguments in Text},
year = {2009},
booktitle = {ICAIL}
}

@inproceedings{pan2021contraqa,
  title={Attacking Open-domain Question Answering by Injecting Misinformation},
  author={Pan, Liangming and Chen, Wenhu and Kan, Min-Yen and Wang, William Yang},
  booktitle={AACL},
  year={2023}
}

@inproceedings{hasan-ng-2014-taking,
    title = "Why are You Taking this Stance? Identifying and Classifying Reasons in Ideological Debates",
    author = "Hasan, Kazi Saidul  and
      Ng, Vincent",
    booktitle = "EMNLP",
    year = "2014",
}

@inproceedings{cabrio-villata-2012-combining,
    title = "Combining Textual Entailment and Argumentation Theory for Supporting Online Debates Interactions",
    author = "Cabrio, Elena  and
      Villata, Serena",
    booktitle = {ACL},
    year = "2012",
}

@inproceedings{chen2019seeing,
  title={Seeing things from a different angle: Discovering diverse perspectives about claims},
  author={Chen, Sihao and Khashabi, Daniel and Yin, Wenpeng and Callison-Burch, Chris and Roth, Dan},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{habernal-gurevych-2016-argument,
    title = "Which argument is more convincing? {Analyzing} and predicting convincingness of Web arguments using bidirectional {LSTM}",
    author = "Habernal, Ivan  and
      Gurevych, Iryna",
    booktitle = "ACL",
    year = "2016",
}

@inproceedings{ovesdotter-alm-2011-subjective,
    title = "Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications",
    author = "Ovesdotter Alm, Cecilia",
    booktitle = {ACL},
    year = {2011},
}

@inproceedings{gleize2019you,
  title={Are you convinced? {Choosing} the more convincing evidence with a Siamese network},
  author={Gleize, Martin and Shnarch, Eyal and Choshen, Leshem and Dankin, Lena and Moshkowich, Guy and Aharonov, Ranit and Slonim, Noam},
  booktitle = {ACL},
  year={2019}
}

@inproceedings{santurkar2023whose,
  title={Whose opinions do language models reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  booktitle={ICML},
  year={2023}
}

@article{Zamani2020GeneratingCQ,
  title={Generating Clarifying Questions for Information Retrieval},
  author={Hamed Zamani and Susan T. Dumais and Nick Craswell and Paul N. Bennett and Gord Lueck},
  journal={The Web Conference},
  year={2020},
}

@inproceedings{longpre2021entity,
  title={Entity-based knowledge conflicts in question answering},
  author={Longpre, Shayne and Perisetla, Kartik and Chen, Anthony and Ramesh, Nikhil and DuBois, Chris and Singh, Sameer},
  booktitle={EMNLP},
  year={2021}
}

@article{wang2023large,
  title={Large language models are not fair evaluators},
  author={Wang, Peiyi and Li, Lei and Chen, Liang and Zhu, Dawei and Lin, Binghuai and Cao, Yunbo and Liu, Qi and Liu, Tianyu and Sui, Zhifang},
  journal={arXiv preprint arXiv:2305.17926},
  year={2023}
}

@misc{openai2023gpt4,
      title={{GPT-4} Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vicuna,
    title = {Vicuna: {An} Open-Source Chatbot Impressing {GPT-4} with 90\%* {ChatGPT} Quality},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    year = {2023}
}

@article{koala,
  author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},
  title = {Koala: {A} Dialogue Model for Academic Research},
  year = {2023},
  journal={{BAIR} Blog}
}


@article{zheng2023judging,
  title={Judging {LLM-as-a-judge} with {MT-Bench} and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@inproceedings{min2020ambigqa,
  title={{AmbigQA}: {Answering} ambiguous open-domain questions},
  author={Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={EMNLP},
  year={2020}
}

@article{sun2023answering,
  title={Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions},
  author={Sun, Haitian and Cohen, William W and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2308.08661},
  year={2023}
}

@article{zhong2023goal,
  title={Goal driven discovery of distributional differences via language descriptions},
  author={Zhong, Ruiqi and Zhang, Peter and Li, Steve and Ahn, Jinwoo and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2302.14233},
  year={2023}
}

@inproceedings{zhong2022describing,
  title={Describing differences between text distributions with natural language},
  author={Zhong, Ruiqi and Snell, Charlie and Klein, Dan and Steinhardt, Jacob},
  booktitle={ICML},
  year={2022},
}

@inproceedings{kandpal2023large,
  title={Large language models struggle to learn long-tail knowledge},
  author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  booktitle={ICML},
  year={2023},}
}

@inproceedings{mallen2022not,
  title={When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Hajishirzi, Hannaneh and Khashabi, Daniel},
  booktitle={ACL},
  year={2023}
}

@article{kazemi2023boardgameqa,
  title={{BoardgameQA}: {A} Dataset for Natural Language Reasoning with Contradictory Information},
  author={Kazemi, Mehran and Yuan, Quan and Bhatia, Deepti and Kim, Najoung and Xu, Xin and Imbrasaite, Vaiva and Ramachandran, Deepak},
  journal={arXiv preprint arXiv:2306.07934},
  year={2023}
}

@article{nakano2021webgpt,
  title={{WebGPT}: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle=cvpr,
  year={2015}
}


@misc{openwebtext,
    author = {Aaron Gokaslan and Vanya Cohen},
    title = {{OpenWebText} corpus},
    year = {2019},
    howpublished = {\url{http://Skylion007.github.io/}},
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@inproceedings{klimt2004enron,
  title={The {Enron} corpus: A new dataset for email classification research},
  author={Klimt, Bryan and Yang, Yiming},
  booktitle=ecml,
  year={2004}
}

@inproceedings{he2016ups,
  title={Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering},
  author={He, Ruining and McAuley, Julian},
  booktitle=www,
  year={2016},
}

@article{drozdov2022you,
  title={You can't pick your neighbors, or can you? When and how to rely on retrieval in the {$k$NN-LM}},
  author={Drozdov, Andrew and Wang, Shufan and Rahimi, Razieh and McCallum, Andrew and Zamani, Hamed and Iyyer, Mo hit},
  journal={arXiv preprint arXiv:2210.15859},
  year={2022}
}

@misc{presser2020books,
    author={Shawn Presser},
    year={2020},
    title={Books3 Corpus},
    url={https://twitter.com/theshawwn/status/1320282149329784833}
}

@inproceedings{lin2021pyserini,
  title={Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations},
  author={Lin, Jimmy and Ma, Xueguang and Lin, Sheng-Chieh and Yang, Jheng-Hong and Pradeep, Ronak and Nogueira, Rodrigo},
  booktitle=sigir,
  year={2021}
}

@article{
kocetkov2023the,
title={The Stack: 3{TB} of permissively licensed source code},
author={Denis Kocetkov and Raymond Li and Loubna Ben allal and Jia LI and Chenghao Mou and Yacine Jernite and Margaret Mitchell and Carlos Mu{\~n}oz Ferrandis and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro Von Werra and Harm de Vries},
journal=tmlr,
year={2023},
}

@inproceedings{
baevski2018adaptive,
title={Adaptive Input Representations for Neural Language Modeling},
author={Alexei Baevski and Michael Auli},
booktitle=iclr,
year={2019},
}


@inproceedings{han2023understanding,
    title = "Understanding In-Context Learning via Supportive Pretraining Data",
    author = "Han, Xiaochuang  and
      Simig, Daniel  and
      Mihaylov, Todor  and
      Tsvetkov, Yulia  and
      Celikyilmaz, Asli  and
      Wang, Tianlu",
    booktitle =acl,
    year = "2023",
}

@inproceedings{izacard2022few,
  title={Atlas: {Few-shot} learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  booktitle={JMLR},
  year={2023},
}

@article{li2022branch,
  title={Branch-train-merge: Embarrassingly parallel training of expert language models},
  author={Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.03306},
  year={2022}
}

@inproceedings{cao2015towards,
  title={Towards making systems forget with machine unlearning},
  author={Cao, Yinzhi and Yang, Junfeng},
  booktitle={2015 IEEE symposium on security and privacy},
  pages={463--480},
  year={2015},
  organization={IEEE}
}

@article{jang2023knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  journal=acl,
  year={2023}
}

@inproceedings{shvartzshnaider2019vaccine,
  title={Vaccine: Using contextual integrity for data leakage detection},
  author={Shvartzshnaider, Yan and Pavlinovic, Zvonimir and Balashankar, Ananth and Wies, Thomas and Subramanian, Lakshminarayanan and Nissenbaum, Helen and Mittal, Prateek},
  booktitle=www,
  year={2019}
}

@article{mireshghallah2022non,
  title={Non-parametric temporal adaptation for social media topic classification},
  author={Mireshghallah, Fatemehsadat and Vogler, Nikolai and He, Junxian and Florez, Omar and El-Kishky, Ahmed and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2209.05706},
  year={2022}
}

@article{flanagin2000credibility,
author = {Andrew J. Flanagin and Miriam J. Metzger},
title ={Perceptions of Internet Information Credibility},
journal = {Journalism \& Mass Communication Quarterly},
year = {2000},
}

@article{kakol2017c3,
title = {Understanding and predicting Web content credibility using the Content Credibility Corpus},
journal = {Information Processing \& Management},
year = {2017},
author = {Michal Kakol and Radoslaw Nielek and Adam Wierzbicki},
}

@article{xie2023adaptive,
  title={Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Conflicts},
  author={Xie, Jian and Zhang, Kai and Chen, Jiangjie and Lou, Renze and Su, Yu},
  journal={arXiv preprint arXiv:2305.13300},
  year={2023}
}

@article{Metzger2010SocialAH,
  title={Social and Heuristic Approaches to Credibility Evaluation Online},
  author={Miriam J. Metzger and Andrew J. Flanagin and Ryan Bradley Medders},
  journal={Journal of Communication},
  year={2010},
}

@inproceedings{jia2017adversarial,
  title={Adversarial examples for evaluating reading comprehension systems},
  author={Jia, Robin and Liang, Percy},
  booktitle=emnlp,
  year={2017}
}

@inproceedings{fogg2003,
author = {Fogg, B. J. and Soohoo, Cathy and Danielson, David R. and Marable, Leslie and Stanford, Julianne and Tauber, Ellen R.},
title = {How Do Users Evaluate the Credibility of Web Sites? A Study with over 2,500 Participants},
year = {2003},
booktitle = {Designing for User Experiences},
}

@inproceedings{yao2022react,
  title={{ReAct}: {Synergizing} reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle={ICLR},
  year={2023}
}

@article{liu2023evaluating,
  title={Evaluating verifiability in generative search engines},
  author={Liu, Nelson F and Zhang, Tianyi and Liang, Percy},
  journal={arXiv preprint arXiv:2304.09848},
  year={2023}
}

@inproceedings{karpukhin2020dense,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={EMNLP},
  year={2020}
}

@misc{adept,
  title={{ACT}-1: Transformer for Actions},
  author={Adept},
  url={https://www.adept.ai/blog/act-1},
  year={2022}
}


@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  booktitle={ICLR},
  year={2022}
}

@misc{claude2,
  title={Claude {2}},
  author={Anthropic},
  url={https://www.anthropic.com/index/claude-2},
  year={2023}
}



@inproceedings{gretz2020large,
  title={A large-scale dataset for argument quality ranking: Construction and analysis},
  author={Gretz, Shai and Friedman, Roni and Cohen-Karlik, Edo and Toledo, Assaf and Lahav, Dan and Aharonov, Ranit and Slonim, Noam},
  booktitle={AAAI},
  year={2020}
}

@inproceedings{Hofstaetter2021_tasb_dense_retrieval,
 author = {Sebastian Hofst{\"a}tter and Sheng-Chieh Lin and Jheng-Hong Yang and Jimmy Lin and Allan Hanbury},
 title = {Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling},
 booktitle = {SIGIR},
 year = {2021},
}

@article{patil2023gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{perez2022discovering,
  title={Discovering language model behaviors with model-written evaluations},
  author={Perez, Ethan and Ringer, Sam and Luko{\v{s}}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and others},
  journal={arXiv preprint arXiv:2212.09251},
  year={2022}
}

@article{liu2023lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={arXiv preprint arXiv:2307.03172},
  year={2023}
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={ICML},
  year={2021},
}

@misc{chatgpt2022,
title={{ChatGPT}: Optimizing Language Models for Dialogue},
author={OpenAI},
year={2022}
}


@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@article{touvron2023llama2,
  title={{LLaMA} 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{suo2024signed,
  title={{Signed-Prompt}: {A} New Approach to Prevent Prompt Injection Attacks Against {LLM}-Integrated Applications},
  author={Suo, Xuchen},
  journal={arXiv preprint arXiv:2401.07612},
  year={2024}
}

@inproceedings{greshake2023not,
  title={Not what you've signed up for: Compromising real-world {LLM}-integrated applications with indirect prompt injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  booktitle={ACM Workshop on Artificial Intelligence and Security},
  year={2023}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{THOMAS2009589,
title = {On automated prepared statement generation to remove {SQL} injection vulnerabilities},
journal = {Information and Software Technology},
year = {2009},
author = {Stephen Thomas and Laurie Williams and Tao Xie},
}



@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  booktitle={EMNLP},
  year={2022}
}

@misc{patronus,
  title={Introducing {CopyrightCatcher}, the first Copyright Detection {API} for {LLMs}},
  author={{Patronus AI}},
  year={2024},
  url={https://www.patronus.ai/blog/introducing-copyright-catcher}
}

@misc{lakera,
  title={Gandalf Game---{Level} 4 Adventure},
  author={{Lakera AI}},
  year={2023},
  url={https://huggingface.co/datasets/Lakera/gandalf_summarization}
}

@misc{openai2024memory,
  title={Memory and new controls for ChatGPT},
  author={OpenAI},
  year={2024},
  url={https://openai.com/blog/memory-and-new-controls-for-chatgpt}
}

@article{liu2023prompt,
  title={Prompt Injection attack against {LLM}-integrated Applications},
  author={Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and Liu, Yang},
  journal={arXiv preprint arXiv:2306.05499},
  year={2023}
}

@misc{willison2022prompt,
  title={Prompt injection attacks against {GPT-3}
},
  author={Simon Willison},
  year={2022},
  url={https://simonwillison.net/2022/Sep/12/prompt-injection/}
}


@misc{willison2022delimeters,
  title={Delimiters won’t save you from prompt injection},
  author={Simon Willison},
  year={2023},
  url={https://simonwillison.net/2023/May/11/delimiters-wont-save-you/}
}


@misc{gptv,
  title={{GPT-4V(ision)} System Card},
  author={OpenAI},
  year={2023},
}

@inproceedings{corbato1965introduction,
  title={Introduction and overview of the {Multics} system},
  author={Corbat{\'o}, Fernando J and Vyssotsky, Victor A},
  booktitle={November 30--December 1, 1965, Fall Joint Computer Conference, Part {I}},
  year={1965}
}

@article{unix,
author = {Ritchie, Dennis M. and Thompson, Ken},
title = {The {UNIX} time-sharing system},
year = {1974},
journal = {Communications of the ACM},
}



@article{team2023gemini,
  title={Gemini: {A} family of highly capable multimodal models},
  author={Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@misc{willison2023multi,
  title={Multi-modal prompt injection image attacks against {GPT-4V}},
  author={Simon Willison},
  year={2023},
  url={https://simonwillison.net/2023/Oct/14/multi-modal-prompt-injection/}
}

@misc{willison2023dual,
  title={The Dual {LLM} pattern for building {AI} assistants that can resist prompt injection},
  author={Simon Willison},
  year={2023},
  url={https://simonwillison.net/2023/Apr/25/dual-llm-pattern/}
}



@misc{zhong2024command,
  title={Command injection | {OWASP} foundation},
  author={Weilin Zhong and Wichers and Amwestgate and Rezos and Clow808 and
KristenS and Jason Li and Andrew Smith and Jmanico and Tal Mel and kingthorin},
  year={2024},
  url={https://owasp.org/wwwcommunity/attacks/Command_Injection}
}

@inproceedings{Vogt2007CrossSS,
  title={Cross Site Scripting Prevention with Dynamic Data Tainting and Static Analysis},
  author={Philipp Vogt and Florian Nentwich and Nenad Jovanovi{\'c} and Engin Kirda and Christopher Kr{\"u}gel and Giovanni Vigna},
  booktitle={Network and Distributed System Security Symposium},
  year={2007},
}

@inproceedings{Su2006TheEO,
  title={The essence of command injection attacks in web applications},
  author={Zhendong Su and Gary Wassermann},
  booktitle={ACM-SIGACT Symposium on Principles of Programming Languages},
  year={2006},
}


@article{yi2023benchmarking,
  title={Benchmarking and defending against indirect prompt injection attacks on large language models},
  author={Yi, Jingwei and Xie, Yueqi and Zhu, Bin and Hines, Keegan and Kiciman, Emre and Sun, Guangzhong and Xie, Xing and Wu, Fangzhao},
  journal={arXiv preprint arXiv:2312.14197},
  year={2023}
}


@inproceedings{zverev2024can,
  title={Can {LLMs} Separate Instructions From Data? {And} What Do We Even Mean By That?},
  author={Zverev, Egor and Abdelnabi, Sahar and Fritz, Mario and Lampert, Christoph H},
  booktitle={{ICLR} 2024 Workshop on Secure and Trustworthy Large Language Models},
  year={2024}
}

@article{piet2023jatmo,
  title={Jatmo: Prompt injection defense by task-specific finetuning},
  author={Piet, Julien and Alrashed, Maha and Sitawarin, Chawin and Chen, Sizhe and Wei, Zeming and Sun, Elizabeth and Alomair, Basel and Wagner, David},
  journal={arXiv preprint arXiv:2312.17673},
  year={2023}
}

@article{yu2023assessing,
  title={Assessing prompt injection risks in 200+ custom gpts},
  author={Yu, Jiahao and Wu, Yuhang and Shu, Dong and Jin, Mingyu and Xing, Xinyu},
  journal={arXiv preprint arXiv:2311.11538},
  year={2023}
}

@article{chen2024struq,
  title={{StruQ}: {Defending} Against Prompt Injection with Structured Queries},
  author={Chen, Sizhe and Piet, Julien and Sitawarin, Chawin and Wagner, David},
  journal={arXiv preprint arXiv:2402.06363},
  year={2024}
}

@article{snell2022learning,
  title={Learning by distilling context},
  author={Snell, Charlie and Klein, Dan and Zhong, Ruiqi},
  journal={arXiv preprint arXiv:2209.15189},
  year={2022}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@article{geiping2024coercing,
  title={Coercing {LLMs} to do and reveal (almost) anything},
  author={Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom},
  journal={arXiv preprint arXiv:2402.14020},
  year={2024}
}


@inproceedings{wei2024jailbroken,
  title={Jailbroken: How does {LLM} safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{schulhoff2023ignore,
  title={Ignore This Title and {HackAPrompt}: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition},
  author={Schulhoff, Sander and Pinto, Jeremy and Khan, Anaum and Bouchard, Louis-Fran{\c{c}}ois and Si, Chenglei and Anati, Svetlina and Tagliabue, Valen and Kost, Anson Liu and Carnahan, Christopher and Boyd-Graber, Jordan},
  booktitle={EMNLP},
  year={2023}
}

@inproceedings{perez2022ignore,
  title={Ignore previous prompt: Attack techniques for language models},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  booktitle={NeurIPS ML Safety Workshop},
  year={2022}
}

@inproceedings{toyer2023tensor,
  title={Tensor {Trust}: {Interpretable} prompt injection attacks from an online game},
  author={Toyer, Sam and Watkins, Olivia and Mendes, Ethan Adrian and Svegliato, Justin and Bailey, Luke and Wang, Tiffany and Ong, Isaac and Elmaaroufi, Karim and Abbeel, Pieter and Darrell, Trevor and others},
  booktitle={ICLR},
  year={2024}
}

@article{yang2024prsa,
  title={PRSA: Prompt Reverse Stealing Attacks against Large Language Models},
  author={Yang, Yong and Zhang, Xuhong and Jiang, Yi and Chen, Xi and Wang, Haoyu and Ji, Shouling and Wang, Zonghui},
  journal={arXiv preprint arXiv:2402.19200},
  year={2024}
}

@article{sha2024prompt,
  title={Prompt Stealing Attacks Against Large Language Models},
  author={Sha, Zeyang and Zhang, Yang},
  journal={arXiv preprint arXiv:2402.12959},
  year={2024}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}


@inproceedings{wallace2019universal,
  title={Universal Adversarial Triggers for Attacking and Analyzing {NLP}},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  booktitle={EMNLP},
  year={2019}
}


@article{zhang2023prompts,
  title={Prompts should not be seen as secrets: Systematically measuring prompt extraction attack success},
  author={Zhang, Yiming and Ippolito, Daphne},
  journal={arXiv preprint arXiv:2307.06865},
  year={2023}
}

@techreport{kincaid1975,
  title={Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel},
  author={Kincaid, J Peter and Fishburne Jr, Robert P and Rogers, Richard L and Chissom, Brad S},
  year={1975},
  institution={Naval Technical Training Command Millington TN Research Branch}
}

@misc{jia2017adversarial,
      title={Adversarial Examples for Evaluating Reading Comprehension Systems}, 
      author={Robin Jia and Percy Liang},
      year={2017},
      eprint={1707.07328},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INPROCEEDINGS{sharma2019seo,
  author={Sharma, Dushyant and Shukla, Rishabh and Giri, Anil Kumar and Kumar, Sumit},
  booktitle={2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)}, 
  title={A Brief Review on Search Engine Optimization}, 
  year={2019},
  volume={},
  number={},
  pages={687-692},
  doi={10.1109/CONFLUENCE.2019.8776976}
}

@article{parisi2022talm,
  title={{TALM}: Tool augmented language models},
  author={Parisi, Aaron and Zhao, Yao and Fiedel, Noah},
  journal={arXiv preprint arXiv:2205.12255},
  year={2022}
}

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{weng2023agent,
  title   = "{LLM}-powered Autonomous Agents",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2023",
  month   = "Jun",
  url     = "https://lilianweng.github.io/posts/2023-06-23-agent/"
}

@article{shen2024hugginggpt,
  title={{HuggingGPT}: Solving {AI} Tasks with {ChatGPT} and its Friends in Hugging Face},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={NeurIPS},
  year={2024}
}
